{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96e1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# For BAAI/bge-large-en-v1.5 model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Vector database\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4a2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DELETE ALL EMBEDDINGS - RUN THIS ONCE\n",
    "# client.delete_collection(name=COLL_NAME)\n",
    "# collection = client.create_collection(name=COLL_NAME)\n",
    "# print(f\"✓ DELETED ALL EMBEDDINGS\")\n",
    "# print(f\"✓ Current count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c48dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/brunamedeiros/Documents/GitHub/agentic-voice-assistant\n",
      "Data path: /Users/brunamedeiros/Documents/GitHub/agentic-voice-assistant/data/homes_preprocessed_data.csv\n",
      "Using open alternative: BAAI/bge-large-en-v1.5\n",
      "Using device: cpu\n",
      "Loaded 708 products from /Users/brunamedeiros/Documents/GitHub/agentic-voice-assistant/data/homes_preprocessed_data.csv\n",
      "Found 'embedding_text' column with 708 non-null values\n",
      "\n",
      "Vector store connected! Current embeddings: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------- Paths Configuration -----------\n",
    "# Get the project root directory\n",
    "# In Jupyter, we detect the project root by looking for common project files\n",
    "current_dir = os.getcwd()\n",
    "if os.path.basename(current_dir) == \"notebooks\":\n",
    "    # If we're in the notebooks directory, go up one level\n",
    "    PROJECT_ROOT = os.path.dirname(current_dir)\n",
    "else:\n",
    "    # If we're in the project root, use current directory\n",
    "    PROJECT_ROOT = current_dir\n",
    "\n",
    "# Data file path\n",
    "DF_PATH = os.path.join(PROJECT_ROOT, \"data\", \"homes_preprocessed_data.csv\") # Alternative simple path (if running from project root): DF_PATH = \"data/homes_preprocessed_data.csv\"\n",
    "\n",
    "# Vector store path\n",
    "VECTOR_STORE_VERSION = \"v1\"\n",
    "VS_PATH = os.path.join(PROJECT_ROOT, f\"vector_store_{VECTOR_STORE_VERSION}\")\n",
    "COLL_NAME = \"agentic_voice_assistant_vdb\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data path: {DF_PATH}\")\n",
    "\n",
    "# ----------- Embedding Model Configuration -----------\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"  \n",
    "EMBEDDING_DIM = 1024\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 100\n",
    "print(\"Using open alternative: BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ----------- Load data -----------\n",
    "def must_exist(p):\n",
    "    assert os.path.exists(p), f\"Missing file: {p}\"\n",
    "\n",
    "must_exist(DF_PATH)\n",
    "os.makedirs(VS_PATH, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(DF_PATH)\n",
    "print(f\"Loaded {len(df)} products from {DF_PATH}\")\n",
    "\n",
    "# Check if embedding_text column exists\n",
    "if \"embedding_text\" not in df.columns:\n",
    "    raise ValueError(\"DataFrame must have 'embedding_text' column. Run data_preprocessing.ipynb first.\")\n",
    "\n",
    "print(f\"Found 'embedding_text' column with {df['embedding_text'].notna().sum()} non-null values\\n\")\n",
    "\n",
    "# ----------- Reconnect to vector store -----------\n",
    "client = chromadb.PersistentClient(\n",
    "    path=VS_PATH,\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "collection = client.get_or_create_collection(name=COLL_NAME)\n",
    "print(f\"Vector store connected! Current embeddings: {collection.count()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba255c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded on cpu\n",
      "Model max sequence length: 512 tokens\n",
      "Your data: avg=265 tokens, max=919 tokens, 95th percentile=546 tokens\n",
      "  Some products (919 tokens) may exceed model limit (512).\n",
      "   Use chunking for longer texts.\n",
      "\n",
      "Max tokens in dataset: 920\n",
      "Model max context: 512\n",
      "Embedding functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ----------- Load Embedding Model -----------\n",
    "print(f\"\\nLoading model: {EMBEDDING_MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME, device_map=device)\n",
    "embedding_model.eval()  # Set to evaluation mode\n",
    "print(f\"✓ Model loaded on {device}\")\n",
    "\n",
    "# Get actual max sequence length from model config\n",
    "if hasattr(embedding_model, 'config'):\n",
    "    actual_max_length = getattr(embedding_model.config, 'max_position_embeddings', MAX_SEQ_LENGTH)\n",
    "    max_seq_length = min(actual_max_length, MAX_SEQ_LENGTH)\n",
    "else:\n",
    "    max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "print(f\"Model max sequence length: {max_seq_length} tokens\")\n",
    "print(f\"Your data: avg=265 tokens, max=919 tokens, 95th percentile=546 tokens\")\n",
    "\n",
    "if max_seq_length >= 1024:\n",
    "    print(f\"✓ Model can handle all products without chunking! ({max_seq_length} > 919)\")\n",
    "else:\n",
    "    print(f\"  Some products (919 tokens) may exceed model limit ({max_seq_length}).\")\n",
    "    print(\"   Use chunking for longer texts.\")\n",
    "print()\n",
    "\n",
    "# Calculate max tokens from your data\n",
    "max_tokens_in_data = df['embedding_text'].apply(\n",
    "    lambda x: len(tokenizer.encode(str(x), add_special_tokens=False)) if pd.notna(x) else 0\n",
    ").max()\n",
    "\n",
    "print(f\"Max tokens in dataset: {max_tokens_in_data}\")\n",
    "print(f\"Model max context: {max_seq_length}\")\n",
    "\n",
    "# Use for chunking\n",
    "CHUNK_SIZE = max_seq_length  # 512 for BGE\n",
    "OVERLAP = 50\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Embedding Functions -----------\n",
    "\n",
    "def get_text_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get embedding for a single text using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        \n",
    "    Returns:\n",
    "        List of normalized embedding values\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        # Return zero vector if empty\n",
    "        return [0.0] * EMBEDDING_DIM\n",
    "    \n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "        \n",
    "        # For BGE models: use mean pooling of last hidden state\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    \n",
    "    # Convert to numpy\n",
    "    import numpy as np\n",
    "    embedding_array = embeddings.cpu().numpy()\n",
    "    \n",
    "    # CRITICAL: Normalize the embedding for cosine similarity\n",
    "    norm = np.linalg.norm(embedding_array)\n",
    "    if norm > 0:\n",
    "        embedding_array = embedding_array / norm\n",
    "    \n",
    "    # Convert to list\n",
    "    embedding_list = embedding_array.tolist()\n",
    "    \n",
    "    # Ensure correct dimension\n",
    "    if len(embedding_list) > EMBEDDING_DIM:\n",
    "        embedding_list = embedding_list[:EMBEDDING_DIM]\n",
    "    elif len(embedding_list) < EMBEDDING_DIM:\n",
    "        embedding_list.extend([0.0] * (EMBEDDING_DIM - len(embedding_list)))\n",
    "    \n",
    "    return embedding_list\n",
    "\n",
    "\n",
    "def create_overlapping_chunks(text: str, max_tokens: int = None, overlap_tokens: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks based on token count.\n",
    "    Uses EmbeddingGemma's tokenizer to ensure chunks fit within token limits.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        max_tokens: Maximum tokens per chunk (defaults to model's max_seq_length)\n",
    "        overlap_tokens: Number of tokens to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    if max_tokens is None:\n",
    "        max_tokens = max_seq_length\n",
    "    \n",
    "    # Tokenize the full text (without special tokens for accurate counting)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # If text fits in one chunk, return as-is\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        # Get chunk of tokens\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Decode back to text\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Move start position with overlap\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "        start = end - overlap_tokens\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_text_embedding_from_chunk(text_chunk: str, product_id: str, chunk_number: int):\n",
    "    \"\"\"\n",
    "    Get xAI/Grok text embedding + metadata for a chunk.\n",
    "    \n",
    "    Args:\n",
    "        text_chunk: Text chunk to embed\n",
    "        product_id: Product ID\n",
    "        chunk_number: Chunk number for this product\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (embedding, metadata) or (None, None) if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if text_chunk and text_chunk.strip():\n",
    "            embedding = get_text_embedding(text_chunk)\n",
    "            \n",
    "            metadata = {\n",
    "                \"product_id\": str(product_id),\n",
    "                \"type\": \"text\",\n",
    "                \"chunk\": chunk_number\n",
    "            }\n",
    "            return embedding, metadata\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing chunk {chunk_number} for {product_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "print(\"Embedding functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b8eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING TEXT EMBEDDING\n",
      "============================================================\n",
      "------------------------------------------------------------\n",
      "EMBEDDING\n",
      "------------------------------------------------------------\n",
      "Stored final batch of 3 text embeddings\n",
      "\n",
      "------------------------------------------------------------\n",
      "Checking results\n",
      "------------------------------------------------------------\n",
      "\n",
      "Total embeddings: 3\n",
      "Text embeddings: 3\n",
      "Products with text embeddings: 3\n",
      "Average chunks per product: 1.0\n",
      "Max chunks for one product: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TESTING TEXT EMBEDDING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_df = df.head(3)\n",
    "\n",
    "embeddings_to_store = []\n",
    "metadatas_to_store = []\n",
    "ids_to_store = []\n",
    "documents_to_store = []\n",
    "\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"EMBEDDING\")\n",
    "print(\"-\"*60)\n",
    "for i in range(len(test_df)):\n",
    "    row = test_df.iloc[i]\n",
    "    \n",
    "    if pd.notna(row['product_id']) and pd.notna(row['embedding_text']):\n",
    "        product_id = row['product_id']\n",
    "        full_text = str(row['embedding_text'])\n",
    "        \n",
    "        # Get chunks for this product\n",
    "        chunks = create_overlapping_chunks(full_text)\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk_num, text_chunk in enumerate(chunks, 1):\n",
    "            \n",
    "            # Create unique ChromaDB ID for each chunk\n",
    "            unique_chroma_id = f\"text_{product_id}_{chunk_num}\"\n",
    "\n",
    "            # Skip if chunk was already embedded\n",
    "            try:\n",
    "                existing = collection.get(ids=[unique_chroma_id])\n",
    "                if existing['ids']: \n",
    "                    print(f\"Skipping {unique_chroma_id} - already exists\")\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Get embedding and metadata\n",
    "            result = get_text_embedding_from_chunk(text_chunk, product_id, chunk_num)\n",
    "            if result is not None:\n",
    "                embedding, metadata = result\n",
    "                if embedding is not None:\n",
    "                    embeddings_to_store.append(embedding)\n",
    "                    metadatas_to_store.append(metadata)\n",
    "                    ids_to_store.append(unique_chroma_id)\n",
    "                    documents_to_store.append(text_chunk)\n",
    "            \n",
    "            # Store every BATCH_SIZE embeddings\n",
    "            if len(embeddings_to_store) >= BATCH_SIZE:\n",
    "                collection.add(embeddings=embeddings_to_store, metadatas=metadatas_to_store, ids=ids_to_store, documents=documents_to_store)\n",
    "                print(f\"Stored batch of {len(embeddings_to_store)} text embeddings\")\n",
    "                # Clear lists for next batch\n",
    "                embeddings_to_store = []\n",
    "                metadatas_to_store = []\n",
    "                ids_to_store = []\n",
    "                documents_to_store = []\n",
    "\n",
    "# Store final batch\n",
    "if embeddings_to_store:\n",
    "    collection.add(embeddings=embeddings_to_store, metadatas=metadatas_to_store, ids=ids_to_store, documents=documents_to_store)\n",
    "    print(f\"Stored final batch of {len(embeddings_to_store)} text embeddings\")\n",
    "\n",
    "print()\n",
    "print(\"-\"*60)\n",
    "print(\"Checking results\")\n",
    "print(\"-\"*60)\n",
    "all_data = collection.get()\n",
    "print(f\"\\nTotal embeddings: {len(all_data['ids'])}\")\n",
    "\n",
    "# Count text embeddings\n",
    "text_embeddings = [id for id in all_data['ids'] if id.startswith('text_')]\n",
    "print(f\"Text embeddings: {len(text_embeddings)}\")\n",
    "\n",
    "# Check text chunks per product\n",
    "text_metadata = [meta for meta in all_data['metadatas'] if meta['type'] == 'text']\n",
    "text_product_ids = [meta['product_id'] for meta in text_metadata]\n",
    "unique_text_product_ids = set(text_product_ids)\n",
    "print(f\"Products with text embeddings: {len(unique_text_product_ids)}\")\n",
    "\n",
    "# Show chunks per product\n",
    "from collections import Counter\n",
    "chunks_per_product = Counter(text_product_ids)\n",
    "print(f\"Average chunks per product: {len(text_product_ids) / len(unique_text_product_ids):.1f}\")\n",
    "print(f\"Max chunks for one product: {max(chunks_per_product.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db79e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL VECTOR STORE VERIFICATION\n",
      "============================================================\n",
      "Collection name: agentic_voice_assistant_vdb\n",
      "Total embeddings: 3\n",
      "Documents stored: True\n",
      "Sample text document: ARTSCAPE Etched Glass 24\" x 36\" Window Film, 24-by-36-Inch Brand: ARTSCAPE Etched Glass Category: ho...\n",
      "Text embeddings: 3\n",
      "\n",
      "Products with text embeddings: 3\n",
      "Sample text IDs: ['text_cc2083338a16c3fe2f7895289d2e98fe_1', 'text_39f1b8a2129315da0288cd058b6b6086_1', 'text_a11d9462309527143094a0f68bce0a58_1']\n",
      "Search test failed: 'list' object has no attribute 'tolist'\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL VERIFICATION CHECK\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL VECTOR STORE VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check collection details\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Total embeddings: {collection.count()}\")\n",
    "\n",
    "# Get all data\n",
    "all_data = collection.get()\n",
    "\n",
    "print(f\"Documents stored: {all_data['documents'] is not None}\")\n",
    "if all_data['documents']:\n",
    "    print(f\"Sample text document: {all_data['documents'][0][:100]}...\")\n",
    "\n",
    "# Count by type\n",
    "text_embeddings = [id for id in all_data['ids'] if id.startswith('text_')]\n",
    "print(f\"Text embeddings: {len(text_embeddings)}\")\n",
    "\n",
    "# Check they have the same product_ids (products)\n",
    "text_product_ids = set([meta['product_id'] for meta in all_data['metadatas'] if meta['type'] == 'text'])\n",
    "print(f\"\\nProducts with text embeddings: {len(text_product_ids)}\")\n",
    "\n",
    "# Sample IDs to verify format\n",
    "print(f\"Sample text IDs: {text_embeddings[:3] if text_embeddings else 'None'}\")\n",
    "\n",
    "# Quick search test to make sure both work\n",
    "try:\n",
    "    query_text = \"longboard skateboard\"\n",
    "    prefixed_query = f\"Represent this sentence for searching relevant passages: {query_text}\"\n",
    "    query_embedding = get_text_embedding(prefixed_query)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],  # 512 dims\n",
    "        n_results=5\n",
    "    )\n",
    "\n",
    "    result_types = [meta['type'] for meta in results['metadatas'][0]]\n",
    "    print(f\"\\nSearch test - found types: {set(result_types)}\")\n",
    "    print(\"Text embeddings are searchable!\")\n",
    "except Exception as e:\n",
    "    print(f\"Search test failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e27efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FULL DATASET TEXT EMBEDDING\n",
      "============================================================\n",
      "------------------------------------------------------------\n",
      "EMBEDDING\n",
      "Skipping text_cc2083338a16c3fe2f7895289d2e98fe_1 - already exists\n",
      "Skipping text_39f1b8a2129315da0288cd058b6b6086_1 - already exists\n",
      "Skipping text_a11d9462309527143094a0f68bce0a58_1 - already exists\n",
      "Stored batch of 100 text embeddings\n",
      "Stored batch of 100 text embeddings\n",
      "Stored batch of 100 text embeddings\n",
      "Stored batch of 100 text embeddings\n",
      "Stored batch of 100 text embeddings\n",
      "Stored batch of 100 text embeddings\n",
      "Stored batch of 100 text embeddings\n",
      "Stored final batch of 50 text embeddings\n",
      "\n",
      "------------------------------------------------------------\n",
      "Checking results\n",
      "------------------------------------------------------------\n",
      "\n",
      "Total embeddings: 753\n",
      "Text embeddings: 753\n",
      "Products with text embeddings: 708\n",
      "Average chunks per product: 1.1\n",
      "Max chunks for one product: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FULL DATASET TEXT EMBEDDING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "embeddings_to_store = []\n",
    "metadatas_to_store = []\n",
    "ids_to_store = []\n",
    "documents_to_store = []\n",
    "\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"EMBEDDING\")\n",
    "for i in range(len(df)): \n",
    "    #row = test_df.iloc[i]\n",
    "    row = df.iloc[i]  \n",
    "    \n",
    "    if pd.notna(row['product_id']) and pd.notna(row['embedding_text']):\n",
    "        product_id = row['product_id']\n",
    "        full_text = str(row['embedding_text'])\n",
    "        \n",
    "        # Get chunks for this product\n",
    "        chunks = create_overlapping_chunks(full_text)\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk_num, text_chunk in enumerate(chunks, 1):\n",
    "            \n",
    "            # Create unique ChromaDB ID for each chunk\n",
    "            unique_chroma_id = f\"text_{product_id}_{chunk_num}\"\n",
    "\n",
    "            # Skip if chunk was already embedded\n",
    "            try:\n",
    "                existing = collection.get(ids=[unique_chroma_id])\n",
    "                if existing['ids']: \n",
    "                    print(f\"Skipping {unique_chroma_id} - already exists\")\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Get embedding and metadata\n",
    "            result = get_text_embedding_from_chunk(text_chunk, product_id, chunk_num)\n",
    "            if result is not None:\n",
    "                embedding, metadata = result\n",
    "                if embedding is not None:\n",
    "                    embeddings_to_store.append(embedding)\n",
    "                    metadatas_to_store.append(metadata)\n",
    "                    ids_to_store.append(unique_chroma_id)\n",
    "                    documents_to_store.append(text_chunk)\n",
    "            \n",
    "            # Store every BATCH_SIZE embeddings\n",
    "            if len(embeddings_to_store) >= BATCH_SIZE:\n",
    "                collection.add(embeddings=embeddings_to_store, metadatas=metadatas_to_store, ids=ids_to_store, documents=documents_to_store)\n",
    "                print(f\"Stored batch of {len(embeddings_to_store)} text embeddings\")\n",
    "                # Clear lists for next batch\n",
    "                embeddings_to_store = []\n",
    "                metadatas_to_store = []\n",
    "                ids_to_store = []\n",
    "                documents_to_store = []\n",
    "\n",
    "# Store final batch\n",
    "if embeddings_to_store:\n",
    "    collection.add(embeddings=embeddings_to_store, metadatas=metadatas_to_store, ids=ids_to_store, documents=documents_to_store)\n",
    "    print(f\"Stored final batch of {len(embeddings_to_store)} text embeddings\")\n",
    "\n",
    "print()\n",
    "print(\"-\"*60)\n",
    "print(\"Checking results\")\n",
    "print(\"-\"*60)\n",
    "all_data = collection.get()\n",
    "print(f\"\\nTotal embeddings: {len(all_data['ids'])}\")\n",
    "\n",
    "# Count text  embeddings\n",
    "text_embeddings = [id for id in all_data['ids'] if id.startswith('text_')]\n",
    "print(f\"Text embeddings: {len(text_embeddings)}\")\n",
    "\n",
    "# Check text chunks per product\n",
    "text_metadata = [meta for meta in all_data['metadatas'] if meta['type'] == 'text']\n",
    "text_product_ids = [meta['product_id'] for meta in text_metadata]\n",
    "unique_text_product_ids = set(text_product_ids)\n",
    "print(f\"Products with text embeddings: {len(unique_text_product_ids)}\")\n",
    "\n",
    "# Show chunks per product\n",
    "from collections import Counter\n",
    "chunks_per_product = Counter(text_product_ids)\n",
    "print(f\"Average chunks per product: {len(text_product_ids) / len(unique_text_product_ids):.1f}\")\n",
    "print(f\"Max chunks for one product: {max(chunks_per_product.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26649d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings: 753\n",
      "Unique text IDs: 753\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify no duplicate chunk IDs\n",
    "text_ids = [id for id in all_data['ids'] if id.startswith('text_')]\n",
    "unique_text_ids = set(text_ids)\n",
    "print(f\"Text embeddings: {len(text_ids)}\")\n",
    "print(f\"Unique text IDs: {len(unique_text_ids)}\")\n",
    "print(f\"Duplicates: {len(text_ids) - len(unique_text_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977e20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1024\n",
      "First 5 values: [-0.030896326526999474, -0.00982965249568224, -0.021214906126260757, -0.017253097146749496, -0.0065686460584402084]\n",
      "Embedding magnitude: 1.0000\n",
      "Min value: -0.1024\n",
      "Max value: 0.1886\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding function\n",
    "test_text = \"coffee maker\"\n",
    "test_embedding = get_text_embedding(test_text)\n",
    "\n",
    "print(f\"Embedding length: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")\n",
    "print(f\"Embedding magnitude: {sum(x**2 for x in test_embedding)**0.5:.4f}\")\n",
    "print(f\"Min value: {min(test_embedding):.4f}\")\n",
    "print(f\"Max value: {max(test_embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf964182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING TEXT EMBEDDING RETRIEVAL\n",
      "============================================================\n",
      "Testing retrieval with query: 'curtains or drapes'\n",
      "\n",
      "Found 3 unique products under threshold 0.6\n",
      "\n",
      "Top results:\n",
      "\n",
      "1. Product ID: cd23e01d89bafecec05b...\n",
      "   Relevance: 73.2%\n",
      "   Distance: 0.5352\n",
      "   Chunk: 1\n",
      "   Preview: Pairs to Go Tiago Window Panel Pair, 60x95, Citron Brand: Pairs Category: home & kitchen Category path: Home & Kitchen Home Décor Kids' Room Décor Win...\n",
      "\n",
      "2. Product ID: 3727a15da01bb547b9ee...\n",
      "   Relevance: 71.7%\n",
      "   Distance: 0.5653\n",
      "   Chunk: 1\n",
      "   Preview: AmazonBasics Kids Room Darkening Blackout Window Curtain Set with Grommets - 42\" x 63\", True Red Brand: AmazonBasics Kids Room Darkening Blackout Wind...\n",
      "\n",
      "3. Product ID: 737fa6ad2a852bebd118...\n",
      "   Relevance: 71.1%\n",
      "   Distance: 0.5773\n",
      "   Chunk: 1\n",
      "   Preview: AmazonBasics Kids Room Darkening Blackout Window Curtain Set with Grommets - 42\" x 84\", Navy Galaxy Brand: AmazonBasics Kids Room Darkening Blackout W...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TESTING TEXT EMBEDDING RETRIEVAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING TEXT EMBEDDING RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test query\n",
    "test_query = \"curtains or drapes\"\n",
    "print(f\"Testing retrieval with query: '{test_query}'\\n\")\n",
    "\n",
    "# Get embedding for query (with BGE prefix for better results)\n",
    "prefixed_query = f\"Represent this sentence for searching relevant passages: {test_query}\"\n",
    "query_embedding = get_text_embedding(prefixed_query)\n",
    "\n",
    "# Search in ChromaDB\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=20,  # Get more candidates to filter\n",
    "    include=[\"metadatas\", \"documents\", \"distances\"]\n",
    ")\n",
    "\n",
    "# Filter by distance threshold\n",
    "DISTANCE_THRESHOLD = 0.6  # Start with 1.0, adjust if needed\n",
    "\n",
    "filtered_results = []\n",
    "seen_products = set()\n",
    "\n",
    "for chunk_id, distance, metadata, doc in zip(\n",
    "    results[\"ids\"][0],\n",
    "    results[\"distances\"][0],\n",
    "    results[\"metadatas\"][0],\n",
    "    results[\"documents\"][0]\n",
    "):\n",
    "    # Only keep results under threshold\n",
    "    if distance > DISTANCE_THRESHOLD:\n",
    "        continue\n",
    "    \n",
    "    product_id = metadata.get('product_id')\n",
    "    \n",
    "    # Deduplicate: only show each product once (best chunk)\n",
    "    if product_id in seen_products:\n",
    "        continue\n",
    "    \n",
    "    seen_products.add(product_id)\n",
    "    filtered_results.append({\n",
    "        \"id\": chunk_id,\n",
    "        \"product_id\": product_id,\n",
    "        \"distance\": distance,\n",
    "        \"relevance\": f\"{(1 - distance/2)*100:.1f}%\",\n",
    "        \"chunk\": metadata.get('chunk', 1),\n",
    "        \"document\": doc\n",
    "    })\n",
    "\n",
    "print(f\"Found {len(filtered_results)} unique products under threshold {DISTANCE_THRESHOLD}\\n\")\n",
    "\n",
    "# Print the FILTERED results\n",
    "if filtered_results:\n",
    "    print(\"Top results:\")\n",
    "    for i, result in enumerate(filtered_results[:10], 1):\n",
    "        print(f\"\\n{i}. Product ID: {result['product_id'][:20]}...\")\n",
    "        print(f\"   Relevance: {result['relevance']}\")\n",
    "        print(f\"   Distance: {result['distance']:.4f}\")\n",
    "        print(f\"   Chunk: {result['chunk']}\")\n",
    "        print(f\"   Preview: {result['document'][:150]}...\")\n",
    "else:\n",
    "    print(\"❌ No results found under the threshold!\")\n",
    "    print(f\"Try increasing DISTANCE_THRESHOLD (current: {DISTANCE_THRESHOLD})\")\n",
    "    print(\"\\nShowing top 5 results regardless of threshold:\")\n",
    "    for i, (chunk_id, distance, metadata, doc) in enumerate(zip(\n",
    "        results[\"ids\"][0][:5],\n",
    "        results[\"distances\"][0][:5],\n",
    "        results[\"metadatas\"][0][:5],\n",
    "        results[\"documents\"][0][:5]\n",
    "    ), 1):\n",
    "        print(f\"\\n{i}. Distance: {distance:.4f}\")\n",
    "        print(f\"   Preview: {doc[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36e785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52032d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"sku\": product_id,\n",
    "  \"title\": product_name,\n",
    "  \"price\": price,\n",
    "  \"rating\": None (none in your dataset),\n",
    "  \"brand\": brand,\n",
    "  \"ingredients\": ingredients,\n",
    "  \"doc_id\": text chunk id,\n",
    "  'shipping_weight_lbs': shipping_weight_lbs, \n",
    "  'model_number': model_number\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. collection.name is  agentic_voice_assistant_vdb\n",
    "2. <yes/no/changes>\n",
    "3. <groq model name>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
